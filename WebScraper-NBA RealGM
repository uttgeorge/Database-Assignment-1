import requests
from urllib.request import urlopen
from bs4 import BeautifulSoup
import pandas as pd
import re
URL = '/nba/stats/2018/Averages/Qualified/points/All/desc/1/Regular_Season'
response = requests.get('https://basketball.realgm.com/nba/stats/2018/Averages/Qualified/points/All/desc/1/Regular_Season')
soup = BeautifulSoup(response.content,'html.parser')
#response = urlopen('https://basketball.realgm.com{}'.format(URL))
#soup = BeautifulSoup(html,'html.parser')
#URL = '/nba/stats/2018/Averages/Qualified/points/All/desc/1/Regular_Season'
head = soup.find('table',{'class':'tablesaw compact'}).find('thead').find('tr')
trnames = head.find_all('th')
names = [th.text for th in trnames]
df = pd.DataFrame(columns=names)
print(df)
pages = set()
pages.add(URL)
def getLinks(myUrl):
    global pages
    global df
    html = urlopen('https://basketball.realgm.com{}'.format(myUrl))
    bs = BeautifulSoup(html,'html.parser')
#href="/nba/stats/2019/Averages/Qualified/points/All/desc/2/Regular_Season"
    table = bs.find('table',{'class':'tablesaw compact'}).find('tbody')
    trs = table.find_all('tr')
    for tr in trs:
        tds = tr.find_all('td')
        row = [td.text for td in tds]
        #.replace('\n','')
        #td.text
        df = df.append(pd.Series(row,index=names),ignore_index=True)    
    for link in bs.find('div',{'class':'main-container'}).find('div').find('p').find_all('a'):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                newPage = link.attrs['href']
                print('-'*20)
                print(newPage)
                pages.add(newPage)
                getLinks(newPage)
         #for link in soup.find('div',id = 'site-takeover').find('div','class'='has_class_but_no_id').find('p').find('a',href=re.compile\
     #     ('^(/nba/stats/2019/Averages/Qualified/points/All/desc/.*/Regular_Season)')):



getLinks(URL)
print(df)
df.to_csv('2017-2018 NBA Stats.csv',index=False)
